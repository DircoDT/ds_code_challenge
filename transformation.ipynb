{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49a8472a-69ed-40d9-a084-d12eb94c8ba3",
   "metadata": {},
   "source": [
    "# Initial Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec209a0-ec8e-481b-8135-629a57af7407",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Library imports\n",
    "\n",
    "Use **pip** or **conda** to install the required packages/libraries before running the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d489b8d-5f07-47de-b6d2-8a40e826e32b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt\n",
    "\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from IPython.display import Markdown\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import h3\n",
    "import h3pandas  # More specialised H3 functions, specifically for use with Pandas\n",
    "\n",
    "# Pandas alternatives (just for speed comparisons)\n",
    "import vaex\n",
    "import dask\n",
    "#import polars\n",
    "#import cudf\n",
    "\n",
    "# Imports for interactive map visualisation.\n",
    "# (Must mark this notebook as Trusted to see it.)\n",
    "import folium\n",
    "from folium.plugins import HeatMap, HeatMapWithTime\n",
    "#import pydeck\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c32fa21-585d-4060-a138-6077a06cd5e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Display settings\n",
    "\n",
    "Set the notebook display option to show all columns in full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5afb6fa-5e96-47da-ad3f-9ba956309b2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55418180-caa5-4ed7-8fc5-8b898fb98308",
   "metadata": {},
   "source": [
    "## Data folders\n",
    "\n",
    "This checks if the data files for the challenge are present, otherwise it downloads them from the S3 bucket.\n",
    "\n",
    "Original files are stored in the **data** folder, while processed output will be written to the **output** folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774a188a-159a-43a3-9727-ad6150752a95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_url = 'https://cct-ds-code-challenge-input-data.s3.af-south-1.amazonaws.com/'\n",
    "\n",
    "data_folder = Path('./data/')\n",
    "output_folder = Path('./output/')\n",
    "\n",
    "data_files = [\n",
    "    'sr.csv.gz',\n",
    "    'sr_hex.csv.gz',\n",
    "    #'sr_hex_truncated.csv',\n",
    "    'city-hex-polygons-8.geojson',\n",
    "    #'city-hex-polygons-8-10.geojson',\n",
    "]\n",
    "\n",
    "# Create `data` and `output` subfolders if they don't exist yet\n",
    "data_folder.mkdir(parents=False, exist_ok=True)\n",
    "output_folder.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "for data_file in data_files:\n",
    "    file_path = data_folder / data_file\n",
    "    if not file_path.exists():\n",
    "        display(Markdown(f'Downloading *{data_file}*...'))\n",
    "        file_url = data_url + data_file\n",
    "        response = requests.get(file_url)\n",
    "        response.raise_for_status()\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        display(Markdown(f'*{data_file}* already exists.'))\n",
    "\n",
    "display(Markdown('**Done.**'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f217e86-a523-46e8-99c6-53913fdd178e",
   "metadata": {},
   "source": [
    "## Schema inspection\n",
    "\n",
    "The first part of the task involves joining the features from `city-hex-polygons-8.geojson` to the service request entries in `sr_hex.csv.gz`.\n",
    "\n",
    "Let's see what the schema looks like by just showing 1 item from each..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb070d7-c13f-4c75-a80a-2cf956748d3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_csv(data_folder / 'sr.csv.gz', nrows=1, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e439e4-ca77-4fce-8bc4-63d779ede49c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpd.read_file(data_folder / 'city-hex-polygons-8.geojson', index_col='index').head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f3752d-00e4-4153-8371-ec05dce51a78",
   "metadata": {},
   "source": [
    "## Calculate the hexagon index\n",
    "\n",
    "There appears to be no common key column that we can join on. We can calculate the H3 hexagon index from the `latitude` and `longitude` of each service request. That can then be matched up with the `index` of the geojson feature.\n",
    "\n",
    "(We could also have done a point-in-polygon check on the polygon geometry, but that would have been much less efficient. Doing a search for the nearest centroid would be similarly inefficient, while also potentially inaccurate for points in the outer corners of the hexagon.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b205726-f5dd-4790-8f93-7a419683ec77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(data_folder / 'sr.csv.gz', index_col=0)\n",
    "\n",
    "# Replace NaN values in the latitude and longitude columns with 0\n",
    "# NOTE: This is optional - h3pandas can work with NaNs.\n",
    "#df[['latitude', 'longitude']] = df[['latitude', 'longitude']].fillna(value=0)\n",
    "\n",
    "# Create a h3pandas DataFrame, calculating the h3 id from the latitude and longitude\n",
    "h3df = df[['latitude', 'longitude']].h3.geo_to_h3(resolution=8, lat_col='latitude', lng_col='longitude')\n",
    "\n",
    "# Append the h3 id column to the original DataFrame\n",
    "df['h3_level8_index'] = h3df.index.to_frame(index=False)\n",
    "\n",
    "# Set the h3 id to '0' if coordinates are missing\n",
    "# NOTE: This is optional - h3pandas already does it internally.\n",
    "#df.loc[df['latitude'].isnull() | df['longitude'].isnull(), 'h3_level8_index'] = 0\n",
    "\n",
    "# Keep for later (validation)\n",
    "df_pandas = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e0884a-b1e8-4e3a-bbaf-89b99573ae85",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Validation\n",
    "\n",
    "Check to make sure our results match the reference CSV file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b37cf6-3c74-461f-8836-4334b49512f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Check if the Pandas DataFrame contains the same values as the provided reference file\n",
    "df_csv = pd.read_csv(data_folder / 'sr_hex.csv.gz', index_col=None)\n",
    "matched = df_pandas.equals(df_csv)\n",
    "display(Markdown(f'Pandas == sr_hex.csv: **{matched}**'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f010ba58-c638-46e5-b07d-6ad6ab3a58bf",
   "metadata": {},
   "source": [
    "## Join on hexagon index\n",
    "\n",
    "Now that we have a common key, we can merge the two DataFrames together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b15962-4069-4014-8230-3195009d05e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Read CSV file into Vaex dataframe\n",
    "df_sr = vaex.from_csv(data_folder / 'sr_hex.csv.gz', index_col=None)\n",
    "#print(len(df_sr))  # 941634\n",
    "\n",
    "# Read GeoJSON file into GeoPandas dataframe\n",
    "df_geo = gpd.read_file(data_folder / 'city-hex-polygons-8.geojson', index_col='index')\n",
    "#print(len(df_geo))  # 3832\n",
    "\n",
    "# Rename key column to match the other dataframe\n",
    "df_sr.rename('h3_level8_index', 'index')\n",
    "\n",
    "# Convert from GeoPandas to Vaex dataframe, depending on what kind of dataframe we want after merging\n",
    "df_geo = vaex.from_pandas(df_geo)\n",
    "#print(type(df_geo))  # <class 'vaex.dataframe.DataFrameLocal'>\n",
    "\n",
    "# Merge the dataframes on the common index column\n",
    "# NOTE: Vaex does not support outer join yet,\n",
    "#       so we do a left join, which keeps all the service requests,\n",
    "#       but discards any unmatched (empty) hexagon features.\n",
    "merged = df_sr.join(df_geo, how='left', on='index', allow_duplication=False)\n",
    "#print(len(merged))  # 941634 (same as service request count)\n",
    "\n",
    "print(f'Column names after merge: { merged.column_names }\\n')\n",
    "#merged.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee2879-7afb-43e9-a389-b6bc9dcf8402",
   "metadata": {},
   "source": [
    "## Find unmatched entries\n",
    "\n",
    "Even without merging, a comparison of the hex id's on both sides will indicate if there are any rows that won't match.\n",
    "\n",
    "1. Check for service requests with hex id's that don't exist in the provided GeoJSON.\n",
    "\n",
    "2. Check for hex id's in the GeoJSON that have no associated service requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89474ecc-3e2e-411a-9d88-2dc817a43b6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Count number of service requests without coordinates\n",
    "num_zero = len(df_sr[df_sr['index']=='0'])\n",
    "\n",
    "# Get list of h3 index strings from both dataframes\n",
    "sr_list = df_sr['index'].tolist()\n",
    "geo_list = df_geo['index'].tolist()\n",
    "\n",
    "# Convert to list to remove duplicates\n",
    "sr_set = set(sr_list)\n",
    "geo_set = set(geo_list)\n",
    "\n",
    "# Only keep non-zero h3 id's\n",
    "sr_set.discard('0')\n",
    "\n",
    "# Get the difference between the sets\n",
    "sr_unmatched = sr_set - geo_set\n",
    "geo_unmatched = geo_set - sr_set\n",
    "\n",
    "# Display some stats\n",
    "display(Markdown(f'## Statistics'))\n",
    "display(Markdown('&nbsp;'))\n",
    "display(Markdown(f'Total service requests: **{ len(sr_list) }**'))\n",
    "display(Markdown(f'- Without coordinates: **{ num_zero }**'))\n",
    "display(Markdown(f'- Inside unlisted hexagons: **{ len(sr_unmatched) }**'))\n",
    "display(Markdown(f'`{ list(sr_unmatched) }`'))\n",
    "display(Markdown('&nbsp;'))\n",
    "display(Markdown(f'Total listed hexagons: **{ len(geo_set) }**'))\n",
    "display(Markdown(f'- With service requests: **{ len(sr_set) }**'))\n",
    "display(Markdown(f'- Without service requests: **{ len(geo_unmatched) }**'))\n",
    "display(Markdown('&nbsp;'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d12d6e-2818-4ef8-93ec-1747b735a49d",
   "metadata": {},
   "source": [
    "## Error threshold\n",
    "\n",
    "It is not entirely clear what the intent is with this part of the challenge instructions:\n",
    "\n",
    "> Include logging that lets the executor know how many of the records failed to join, and include a join error threshold above which the script will error out. Please motivate why you have selected the error threshold that you have.\n",
    "\n",
    "Maybe it's for SQL solutions?\n",
    "Even a 100% failure rate doesn't necessarily mean there is a problem with the script. People might just be leaving out the coordinates when creating service requests, which would lead to an unjoined row that will not be shown on the map. Instead of triggering errors, these incomplete entries could instead be flagged for clean-up before even starting the merge.\n",
    "\n",
    "## Distributed computing\n",
    "\n",
    "Vaex already makes it possible to process the data in chunks, or across multiple processes on a single computer. But if the idea is to split the work across multiple machines on a network, we can use a Dask cluster. The syntax is similar to Pandas and Vaex. It can even utilise the GPU (with RAPIDS cuDF).\n",
    "\n",
    "For now, we stick with just Vaex, though.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43501e54-76b4-43cf-87b7-7f0e8879b739",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Grouping / Aggregation\n",
    "\n",
    "Show the total number of service requests per:\n",
    "- Suburb (`official_suburb`)\n",
    "- Directorate (`directorate`)\n",
    "- Month (new column)\n",
    "- hexagon (`index`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639b0404-c285-4aa6-99e4-3a81e2f5009e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add calculated MMM YYYY column\n",
    "\n",
    "# Define a function to extract month name and year from timestamp string\n",
    "def get_month_name(timestamp_str):\n",
    "    timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S%z')\n",
    "    return timestamp.strftime('%b %Y')\n",
    "\n",
    "# Create a new column called 'month_name' containing the short month name and year format\n",
    "merged['month_name'] = merged['creation_timestamp'].apply(get_month_name)\n",
    "\n",
    "# List of months\n",
    "print(set(merged['month_name'].tolist()))\n",
    "\n",
    "# List of directorates\n",
    "print(set(merged['directorate'].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead9602f-8efb-410d-89c6-e45bbc7278e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Service requests by suburb for specific directorate and month\n",
    "\n",
    "df_only_water = merged[merged['directorate'] == 'WATER AND SANITATION']\n",
    "\n",
    "df_only_recent = df_only_water[df_only_water['month_name'] == 'Dec 2020']\n",
    "\n",
    "agg = df_only_recent.groupby(['official_suburb']).agg({'per_suburb': 'count'})\n",
    "\n",
    "agg = agg.sort('per_suburb', ascending=False)\n",
    "\n",
    "agg.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb40a947-4972-4fbb-a747-ec7aff62fafd",
   "metadata": {},
   "source": [
    "## Preliminary visualisations\n",
    "\n",
    "Some plots and charts to get a sense of the data before building a proper dashboard.\n",
    "\n",
    "First prepare all the hexagon shapes, then all the individual points. Then plot it all on a Folium map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e321d-c6bb-4ad5-9baf-caebfefd9986",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged\n",
    "#df = df_only_water\n",
    "\n",
    "# Use only rows with valid hexagon id\n",
    "df = df[df['index']!='0']\n",
    "\n",
    "# Convert from Vaex to Pandas dataframe\n",
    "df = df.to_pandas_df()\n",
    "\n",
    "# Convert from Pandas to GeoPandas dataframe\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "\n",
    "# Group by h3 hexagon and add a column for number of entries per hexagon\n",
    "grouped_gdf = gdf.groupby(['index']).size().reset_index(name='h3_sr_count')\n",
    "\n",
    "# Merge columns we want to plot\n",
    "gdf_with_count = gdf[['index', 'geometry']].drop_duplicates(subset=['index']).merge(grouped_gdf)\n",
    "\n",
    "# Concatenate with unmatched \n",
    "gdf_unmatched = gpd.GeoDataFrame(df_geo[df_geo['index'].isin(geo_unmatched)][['index', 'geometry']].to_pandas_df(), geometry='geometry')\n",
    "gdf_unmatched['h3_sr_count'] = 0  # Add a h3_sr_count column of 0 for all\n",
    "gdf_final = pd.concat([gdf_with_count, gdf_unmatched], ignore_index=True)\n",
    "\n",
    "gdf_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f688c7a3-70ff-4022-a486-7a3177d29999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only relevant service request columns\n",
    "datapoints = df_sr[['notification_number', 'cause_code', 'latitude', 'longitude']].to_pandas_df()\n",
    "\n",
    "# Only keep points with coordinates\n",
    "datapoints = datapoints.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "# Replace latitude and longitude with a Point geometry\n",
    "datapoints['geometry'] = datapoints.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)\n",
    "\n",
    "# Convert GeoPandas dataframe\n",
    "datapoints = gpd.GeoDataFrame(datapoints, geometry='geometry')\n",
    "\n",
    "# Set CRS (Coordinate Reference System) to WGS 84 datum\n",
    "datapoints = datapoints.set_crs(epsg=4326)\n",
    "\n",
    "datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3df37e-aa3b-497f-91e2-4319868cf2cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate centerpoint before setting CRS\n",
    "mean_latlon = [gdf_final.geometry.centroid.y.mean(), gdf_final.geometry.centroid.x.mean()]\n",
    "\n",
    "# Set CRS (Coordinate Reference System) to WGS 84 datum\n",
    "gdf_crs = gdf_final.set_crs(epsg=4326)\n",
    "\n",
    "# Normalise count values (0..1)\n",
    "#gdf_crs['normalised_count'] = gdf_crs['h3_sr_count']/gdf_crs['h3_sr_count'].max()\n",
    "\n",
    "# Create a map object\n",
    "m = folium.Map(location=mean_latlon, zoom_start=10, tiles='cartodbpositron')\n",
    "\n",
    "# Add choropleth layer(s)\n",
    "choropleth0 = folium.Choropleth(\n",
    "        geo_data=gdf_crs[gdf_crs['h3_sr_count']==0],\n",
    "        name='choropleth0',\n",
    "        data=gdf_crs[gdf_crs['h3_sr_count']==0],\n",
    "        columns=['index', 'h3_sr_count'],\n",
    "        key_on='feature.properties.index',\n",
    "        fill_color='Greys',  # ColorBrewer code\n",
    "        fill_opacity=0.5,\n",
    "        line_opacity=0.5,\n",
    "        legend_name='No Service Requests'\n",
    "    )\n",
    "choropleth1 = folium.Choropleth(\n",
    "        geo_data=gdf_crs[gdf_crs['h3_sr_count']>0],\n",
    "        name='choropleth1',\n",
    "        data=gdf_crs[gdf_crs['h3_sr_count']>0],\n",
    "        columns=['index', 'h3_sr_count'],\n",
    "        key_on='feature.properties.index',\n",
    "        fill_color='YlOrRd',  # ColorBrewer code\n",
    "        fill_opacity=0.5,\n",
    "        line_opacity=0.5,\n",
    "        legend_name='Service Request Count'\n",
    "    )\n",
    "\n",
    "# Add the created layers to the map\n",
    "choropleth0.add_to(m)\n",
    "choropleth1.add_to(m)\n",
    "\n",
    "# Create a marker per individual service request\n",
    "#markers = folium.GeoJson(datapoints.to_json(), name='Points', tooltip=folium.features.GeoJsonTooltip(fields=['cause_code']))  # Too slow...\n",
    "#markers.add_to(m)\n",
    "\n",
    "# Add a heatmap layer instead of individual points\n",
    "HeatMap(datapoints[['latitude', 'longitude']],\n",
    "        radius=3,\n",
    "        blur=5,\n",
    "        min_opacity=1,\n",
    "        gradient={0: 'blue', 0.25: 'cyan', 0.5: 'yellow', 0.75: 'orange', 1: 'red'},\n",
    "        overlay=True,\n",
    "        control=True,\n",
    "        show=True,\n",
    "        name='heatmap',\n",
    "        legend_name='Service Request Heatmap'\n",
    "       ).add_to(m)\n",
    "\n",
    "# Add additional style/theme options.\n",
    "#folium.TileLayer('OpenStreetMap', control=True).add_to(m)\n",
    "#folium.TileLayer('Stamen Toner').add_to(m)\n",
    "#folium.TileLayer('Stamen Terrain').add_to(m)\n",
    "#folium.TileLayer('Stamen Water Color').add_to(m)\n",
    "#folium.TileLayer('cartodbpositron').add_to(m)\n",
    "#folium.TileLayer('cartodbdark_matter').add_to(m)\n",
    "\n",
    "# Make it toggleable\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14247974-7666-472b-931c-e11bc8c7125c",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
